## 1. Сформулируйте задачу классификации.

**Задача классификации** - это задача прогнозирования категориальной (дискретной) метки на основе входных признаков.

**Формальная постановка:**
Дано:
- Множество объектов X = {x₁, x₂, ..., xₙ}
- Множество меток Y = {y₁, y₂, ..., yₖ}
- Обучающая выборка: пары (xᵢ, yᵢ)

Найти: функцию f: X → Y, которая наилучшим образом предсказывает метку для новых объектов.

**Пример:** Определить, является ли email спамом (Y = {спам, не спам}) на основе его содержимого.

## 2. Перечислите типы классификации.

**Основные типы классификации:**

1. **Бинарная классификация** - 2 класса
2. **Мультиклассовая классификация** - 3 и более классов (один правильный)
3. **Многометочная классификация** - несколько меток одновременно
4. **Многоклассовая с пересечениями** - объект может принадлежать нескольким классам

## 3. В чем особенность несбалансированной классификации?

**Особенности несбалансированной классификации:**
- Классы представлены неравномерно (например, 95% класса A и 5% класса B)
- Модель может достигать высокой accuracy, предсказывая только мажоритарный класс
- Требует специальных подходов к оценке и обучению

**Методы решения:**
- Взвешивание классов в функции потерь
- Oversampling миноритарного класса (SMOTE)
- Undersampling мажоритарного класса
- Использование метрик: F1-score, Precision, Recall вместо Accuracy

## 4. В чем особенность мультиклассовой классификации?

**Особенности мультиклассовой классификации:**
- Более 2 классов (Y = {A, B, C, ...})
- Каждый объект принадлежит ровно одному классу
- Требует стратегий для бинарных классификаторов
- Пример: классификация изображений на {кошка, собака, птица}

**Подходы:**
- One-vs-Rest (OvR)
- One-vs-One (OvO)

## 5. В чем особенность бинарной классификации?

**Особенности бинарной классификации:**
- Ровно 2 класса (Y = {0, 1} или {-1, +1})
- Самый простой и распространенный тип
- Позволяет использовать пороговые значения
- Пример: спам/не спам, болен/здоров

**Метрики:**
- Accuracy, Precision, Recall, F1-score
- ROC-AUC curve

## 6. В чем особенность классификации по нескольким меткам?

**Особенности многометочной классификации:**
- Объект может иметь несколько меток одновременно
- Каждая метка предсказывается независимо
- Пример: классификация текстов по темам {политика, экономика, спорт}

**Подходы:**
- Binary Relevance (каждая метка - отдельная бинарная задача)
- Classifier Chains
- Neural networks with multiple outputs

## 7. Чем стратегия "Один против всех" отличается от "Один против одного"?

**One-vs-Rest (OvR) - "Один против всех":**
- Для K классов строится K бинарных классификаторов
- Каждый классификатор отличает один класс от всех остальных
- Прогноз: класс с наибольшей уверенностью
- **Плюсы:** меньше моделей, быстрее обучение
- **Минусы:** может быть несбалансированность

**One-vs-One (OvO) - "Один против одного":**
- Для K классов строится K×(K-1)/2 бинарных классификаторов
- Каждый классификатор для пары классов
- Прогноз: голосование всех классификаторов
- **Плюсы:** более точные границы решений
- **Минусы:** больше моделей, медленнее обучение

## 8. Что такое матрица ошибок (несоответствий)?

**Матрица ошибок (Confusion Matrix)** - таблица для оценки качества классификации, показывающая соотношение предсказанных и истинных меток.

Для бинарной классификации:

| | Предсказано: 0 | Предсказано: 1 |
|---|---|---|
| **Истинно: 0** | TN (True Negative) | FP (False Positive) |
| **Истинно: 1** | FN (False Negative) | TP (True Positive) |

**Элементы:**
- **TP** - правильно предсказанные положительные
- **TN** - правильно предсказанные отрицательные  
- **FP** - ложно положительные (ошибка I рода)
- **FN** - ложно отрицательные (ошибка II рода)

## 9. Как рассчитывается Accuracy?

**Accuracy (точность)** - доля правильных предсказаний среди всех предсказаний.

**Формула:**
```
Accuracy = (TP + TN) / (TP + TN + FP + FN)
```

**Пример:**
- TP = 80, TN = 15, FP = 5, FN = 10
- Accuracy = (80 + 15) / (80 + 15 + 5 + 10) = 95 / 110 ≈ 0.864 (86.4%)

**Ограничения:**
- Не подходит для несбалансированных данных
- Может вводить в заблуждение при неравномерном распределении классов

## 10. Объясните алгоритм классификации K-ближайших соседей.

**K-Nearest Neighbors (K-NN)** - алгоритм, основанный на схожести объектов.

**Принцип работы:**
1. Запоминает все обучающие данные
2. Для нового объекта находит K ближайших соседей (по расстоянию)
3. Присваивает класс по majority vote (большинство голосов)

**Шаги алгоритма:**
1. Выбрать метрику расстояния (евклидово, манхэттенское и т.д.)
2. Выбрать число соседей K
3. Для тестового объекта:
   - Вычислить расстояния до всех обучающих объектов
   - Выбрать K объектов с наименьшими расстояниями
   - Присвоить класс, который чаще всего встречается среди K соседей

**Гиперпараметры:**
- K (количество соседей)
- Метрика расстояния
- Веса соседей (равные или по расстоянию)

**Преимущества:**
- Простота реализации
- Нет этапа обучения
- Легко интерпретировать

**Недостатки:**
- Медленный прогноз для больших данных
- Чувствительность к шуму
- Требует хранения всех данных